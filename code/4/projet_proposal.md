In this project the naive bayes classifier will be compared to 3-5 other classifiers in terms of performance. The data sets will be taken from the UCI Machine Learning Repository [1].
After the performances are measured, instabilities will be figured out. We will investigate the data sets and if there are instabilities in the performances comparing the different data sets, we'll try to explain the outcome of the naive bayes classifier.

As mentioned above we take data sets from the UCI Machine Learning Repository. There you find data sets of very different kinds. We'll select 10 datasets which are all different in their kinds to have a wide spread comparison. If the learners perform uncommon in several data sets, we'll figure out what them makes different.

In [1] the Bayes Classifier is compared to Gauss, C4.5, PEBLS,CN2 and Def. We will compare the Bayes Classifier to J48, ZeroR ,....??. In the paper out projects is based from, the Bayes Classifier performs better than the other classifier. We'll verify that result.
In addition to the measurements of precision, recall and accuracy, we'll also measure the computing time each classifier was used to taken.
After that we compare the quality results and the time results to describe the efficiency of the Naive Bayes Classifier in comparison to the others.

Pazzani et al give explanations to the result of their measurements. We'll try to explain our results among their results, if they fit each other and we'll try to find explanations of our results don't fit into their.





[1]http://archive.ics.uci.edu/ml/
