Reading 4
http://www.sciencedirect.com/science/article/pii/S0167947312003490
<h1>A new variable selection approach using Random Forests
A. Hapfelmeier, , 
K. Ulm
</h1>


The above paper cited this:

<h2>Data mining in the Life Sciences with Random Forest: a walk in the park or lost in the jungle?
Wouter G. Touw, Jumamurat R. Bayjanov, Lex Overmars, Lennart Backus, Jos Boekhorst, Michiel Wels and Sacha A. F. T. van Hijum
 form) : 26th May 2012</h2>

<h3>The four most important keywords:</h3>
<ul>
<li>ii1 Variable selection: It is the process of selecting a subset of relevant features (variables, predictors) for use in model construction.
</li><li>ii2 Multiple testing refers to any instance that involves the simultaneous testing of more than one hypothesis. If decisions about the individual hypotheses are based on the unadjusted marginal p-values, then there is typically a large probability that some of the true null hypotheses will be rejected.
</li><li>ii3 Permutation tests is an increasingly common statistical tool for constructing sampling distributions. Like bootstrapping, a permutation test builds - rather than assumes - sampling distribution by resampling the observed data.
</li><li>ii4 Recursive partitioning is the step-by-step process by which a decision tree is constructed by either splitting or not splitting each node on the tree into two daughter nodes.
</li></ul>

<h3>Brief Description</h3>
<ul><li>iii1 Motivational Statement:
Several approaches for variable selection exist in the literature, and they have been proposed here in a combined manner. An extensive review of the corresponding literature led to the development of a new approach that is based on the theoretical framework of permutation tests and meets important statistical properties, and can be applied to regression and classification problems.
</li><li> iii2 Sampling Procedures: They took four famous datasets so that they are accessible to everyone who wants to reproduce the results in the future. They selected four data sets, two regression datasets and two classification datasets. They datasets were chosen such that they had differing number of variables and observations.
</li><li> iii3 Data :Four well known datasets with differing numbers of variables and observations were chosen for the study:
The Infant Birth Weight Data:  It contains physical measures and information about the health condition of women giving birth
The Boston Housing Data: The data consists of 506 observations and 13 independent variables related to boston housing.
The Heart Disease Data: The data contains 270 observations and 13 independent variables related to heart diseases
The Parkinson’s Disease Detection Dataset: It contains voice recordings of several healthy people and people suffering from Parkinson’s disease. In total there are 22 independent variables measured on 195 observations
</li><li> iii4 Baseline results: To prove generalizability,  the benefit of its application, concerning prediction accuracy and especially the selection of relevant variables, has to be further investigated. Further investigations need to be done to explore factors like correlation strength, block size, interactions, type of importance measure, definition of ‘relevance’ and kind of alpha adjustment which were identified to affect variable selection.
</li></ul>

<h3>Improve paper</h3>
<ul><li>iv1 They should mention the disadvantages of this new approach compared to the previous methods.
</li><li> iv2 The figures are just briefly explained and they are shown earlier than their explanation.
</li><li> iv3 They can explain why that algorithms was working better than the previous variable selection approaches.
</li><li> iv4 The best practices for using the new approach as well as best data sets to use the new approach can be explained. And where this method will not work should be explained, and why it will not work in certain situations can also be explained.
</li></ul>
